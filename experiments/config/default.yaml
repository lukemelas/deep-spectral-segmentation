# @package _global_
hydra:
  run:
    dir: ./outputs/${name}/${now:%Y-%m-%d--%H-%M-%S}

name: "debug"
seed: 1
job_type: 'train'
fp16: False
cpu: False
wandb: False

data:
  num_classes: 1000
  train:
    root: ${oc.env:IMAGENET_DIR}/val
  val:
    root: ${oc.env:IMAGENET_DIR}/val
  loader:
    batch_size: 256
    num_workers: 32
    pin_memory: True
  transform:
    resize_size: 256
    crop_size: 224
    img_mean: [0.485, 0.456, 0.406]
    img_std: [0.229, 0.224, 0.225]

model:
  num_classes: ${data.num_classes}
  pretrained: False

checkpoint:
  resume: null
  resume_training: True
  resume_model_from_ema: False

# Exponential moving average of model parameters
ema:
  enabled: True
  decay: 0.999  # 0.99996 for EfficientNet
  device: 'cpu'
  update_every: 100

# Training steps/epochs
max_train_steps: 100_000
max_train_epochs: null

# Optimization
lr: 0.005
gradient_accumulation_steps: 1
optimizer:
  scale_learning_rate_with_batch_size: False
  clip_grad_norm: null

  # Transformers optimizer
  kind: 'transformers'
  name: 'Adafactor'
  kwargs:
      weight_decay: 0.0
      scale_parameter: True
      relative_step: False

  # # Timm optimizer
  # kind: 'timm'
  # kwargs:
  #   optimizer_name: 'AdamW'
  #   weight_decay: 1e-8

# Learning rate scheduling
scheduler:

  # Transformers scheduler
  kind: 'transformers'
  stepwise: True
  kwargs:
    name: linear
    num_warmup_steps: 0
    num_training_steps: ${max_train_steps}

  # # Timm scheduler
  # kind: 'timm'
  # stepwise: False
  # kwargs:
  #   epochs: ${epochs}
  #   sched: 'cosine'
  #   min_lr: 1e-5
  #   warmup_lr: 1e-6
  #   warmup_epochs: 5
  #   cooldown_epochs: 10
  #   decay_rate: 0.1

logging:
  print_freq: 50
