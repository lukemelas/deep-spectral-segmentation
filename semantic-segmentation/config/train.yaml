# @package _global_
defaults:
  - base
  - _self_

name: 'debug'
job_type: 'train'
fp16: False
wandb: True
eval_every: 1
checkpoint_at: [2, 4, 8, 16, 32, 48, 64, 80, 96, 128, 160, 224]

unfrozen_backbone_layers: 1  # -1 to train all, 0 to freeze entirely, > 0 to specify
model: 
  name: resnet50  # vits16
  num_classes: ${data.num_classes}

lambda_contrastive: 0.2

segments_dir: ""  # ${oc.env:FOUND_NEW_DIR}/data/VOC2012/semantic_segmentations/crf/fixed_15/segmaps_e2_d5_pca_0_s12
matching: ""  # "[(0, 0) (1, 18) (2, 2) (3, 12) (4, 13) (5, 1) (6, 8) (7, 15) (8, 4) (9, 3) (10, 20) (11, 9) (12, 10) (13, 16) (14, 19) (15, 14) (16, 5) (17, 17) (18, 7) (19, 6) (20, 11)]"

checkpoint:
  resume: null
  resume_training: True
  resume_optimizer_only: False

# Exponential moving average of model parameters
ema:
  use_ema: False  # New! Check if this makes any difference
  decay: 0.999
  update_every: 10

# Training steps/epochs
max_train_steps: 3_500  # 5_000  # 20_000
max_train_epochs: null

# Optimization
lr: 0.005
gradient_accumulation_steps: 1
optimizer:
  scale_learning_rate_with_batch_size: False
  clip_grad_norm: null

  # Timm optimizer
  kind: 'timm'
  kwargs:
    optimizer_name: 'AdamW'
    weight_decay: 1e-8

# Learning rate scheduling
scheduler:

  # Transformers scheduler
  kind: 'transformers'
  stepwise: True
  kwargs:
    name: linear
    num_warmup_steps: 0
    num_training_steps: ${max_train_steps}
