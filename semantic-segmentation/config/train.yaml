# @package _global_
defaults:
  - base
  - _self_

name: 'debug'
job_type: 'train'
eval_every: 2
checkpoint_at: [2, 4, 8, 16, 32, 48, 64, 80, 96, 128, 160, 224]

unfrozen_backbone_layers: 1  # -1 to disable
model: 
  num_classes: ${data.num_classes}

checkpoint:
  resume: null
  resume_training: True
  resume_optimizer_only: False

# Exponential moving average of model parameters
ema:
  use_ema: False
  decay: 0.999  # 0.99996 for EfficientNet
  update_every: 100

# Training steps/epochs
max_train_steps: 50_000
max_train_epochs: null

# Optimization
lr: 0.005
gradient_accumulation_steps: 1
optimizer:
  scale_learning_rate_with_batch_size: False
  clip_grad_norm: null

  # Timm optimizer
  kind: 'timm'
  kwargs:
    optimizer_name: 'AdamW'
    weight_decay: 1e-8

# Learning rate scheduling
scheduler:

  # Transformers scheduler
  kind: 'transformers'
  stepwise: True
  kwargs:
    name: linear
    num_warmup_steps: 0
    num_training_steps: ${max_train_steps}
