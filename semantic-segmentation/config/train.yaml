# @package _global_
defaults:
  - base
  - _self_

name: 'debug'
job_type: 'train'
fp16: False
wandb: True
eval_every: 1
checkpoint_at: [2, 4, 8, 16, 32, 48, 64, 80, 96, 128, 160, 224]

unfrozen_backbone_layers: 1  # -1 to train all, 0 to freeze entirely, > 0 to specify
model: 
  name: resnet50  # vits16
  num_classes: ${data.num_classes}

lambda_contrastive: 0.2

checkpoint:
  resume: null
  resume_training: True
  resume_optimizer_only: False

# Exponential moving average of model parameters
ema:
  use_ema: False  # New! Check if this makes any difference
  decay: 0.999
  update_every: 10

# Training steps/epochs
max_train_steps: 3_500  # 5_000  # 20_000
max_train_epochs: null

# Optimization
lr: 0.005
gradient_accumulation_steps: 1
optimizer:
  scale_learning_rate_with_batch_size: False
  clip_grad_norm: null

  # Timm optimizer
  kind: 'timm'
  kwargs:
    optimizer_name: 'AdamW'
    weight_decay: 1e-8

# Learning rate scheduling
scheduler:

  # Transformers scheduler
  kind: 'transformers'
  stepwise: True
  kwargs:
    name: linear
    num_warmup_steps: 0
    num_training_steps: ${max_train_steps}
